"""
Advanced Causal Indicator Integration Module - State-of-the-art causal inference

This module implements cutting-edge causal inference methods including:
- Formal causal discovery algorithms (PC, FCI, LiNGAM)
- Double Machine Learning for causal effect estimation
- Non-linear causal models and time-varying effects
- Instrumental variable estimation
- Synthetic control methods
- Advanced intervention analysis with change point detection
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
import logging
import os
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import networkx as nx
import warnings
warnings.filterwarnings('ignore')

# Configure logger
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CausalDiscoveryEngine:
    """Advanced causal discovery using multiple algorithms"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.causal_graph = None
        
    def pc_algorithm(self, data: pd.DataFrame, alpha: float = 0.05) -> nx.Graph:
        """
        Implement PC (Peter-Clark) algorithm for causal discovery
        
        Args:
            data: DataFrame with variables
            alpha: Significance level for independence tests
            
        Returns:
            Undirected graph representing causal structure
        """
        try:
            # Try using causal-learn if available
            from causallearn.search.ConstraintBased.PC import pc
            from causallearn.utils.cit import CIT
            
            # Convert to numpy array
            data_array = data.select_dtypes(include=[np.number]).values
            
            # Run PC algorithm
            cg = pc(data_array, alpha=alpha, indep_test=CIT(data_array, "fisherz"))
            
            # Convert to networkx graph
            graph = nx.Graph()
            for i, col in enumerate(data.select_dtypes(include=[np.number]).columns):
                graph.add_node(col)
            
            # Add edges from adjacency matrix
            adj_matrix = cg.G.graph
            for i in range(len(adj_matrix)):
                for j in range(i+1, len(adj_matrix)):
                    if adj_matrix[i, j] == 1:
                        col_i = data.select_dtypes(include=[np.number]).columns[i]
                        col_j = data.select_dtypes(include=[np.number]).columns[j]
                        graph.add_edge(col_i, col_j)
            
            return graph
            
        except ImportError:
            logger.warning("causal-learn not available, using simplified PC algorithm")
            return self._simplified_pc_algorithm(data, alpha)
        except Exception as e:
            logger.warning(f"PC algorithm failed: {str(e)}, using simplified version")
            return self._simplified_pc_algorithm(data, alpha)
    
    def _simplified_pc_algorithm(self, data: pd.DataFrame, alpha: float = 0.05) -> nx.Graph:
        """Simplified PC algorithm implementation"""
        numeric_data = data.select_dtypes(include=[np.number])
        graph = nx.Graph()
        
        # Add all nodes
        for col in numeric_data.columns:
            graph.add_node(col)
        
        # Test pairwise independence
        for i, col1 in enumerate(numeric_data.columns):
            for j, col2 in enumerate(numeric_data.columns[i+1:], i+1):
                # Calculate correlation and p-value
                corr, p_value = stats.pearsonr(numeric_data[col1].dropna(), 
                                             numeric_data[col2].dropna())
                
                # Add edge if significantly correlated
                if p_value < alpha and abs(corr) > 0.1:
                    graph.add_edge(col1, col2, weight=abs(corr))
        
        return graph
    
    def lingam_discovery(self, data: pd.DataFrame) -> nx.DiGraph:
        """
        Linear Non-Gaussian Acyclic Model (LiNGAM) for causal discovery
        
        Args:
            data: DataFrame with variables
            
        Returns:
            Directed graph representing causal structure
        """
        try:
            from lingam import DirectLiNGAM
            
            # Prepare data
            numeric_data = data.select_dtypes(include=[np.number]).dropna()
            
            # Fit LiNGAM model
            model = DirectLiNGAM(random_state=self.random_state)
            model.fit(numeric_data.values)
            
            # Create directed graph
            graph = nx.DiGraph()
            columns = numeric_data.columns
            
            # Add nodes
            for col in columns:
                graph.add_node(col)
            
            # Add edges based on adjacency matrix
            adj_matrix = model.adjacency_matrix_
            for i in range(len(adj_matrix)):
                for j in range(len(adj_matrix)):
                    if abs(adj_matrix[i, j]) > 0.1:  # Threshold for significance
                        graph.add_edge(columns[j], columns[i], weight=abs(adj_matrix[i, j]))
            
            return graph
            
        except ImportError:
            logger.warning("lingam package not available, using correlation-based directed graph")
            return self._correlation_based_dag(data)
        except Exception as e:
            logger.warning(f"LiNGAM failed: {str(e)}, using fallback method")
            return self._correlation_based_dag(data)
    
    def _correlation_based_dag(self, data: pd.DataFrame) -> nx.DiGraph:
        """Fallback method using correlation and temporal ordering"""
        numeric_data = data.select_dtypes(include=[np.number])
        graph = nx.DiGraph()
        
        # Add all nodes
        for col in numeric_data.columns:
            graph.add_node(col)
        
        # Add edges based on correlation strength
        for col1 in numeric_data.columns:
            for col2 in numeric_data.columns:
                if col1 != col2:
                    corr = numeric_data[col1].corr(numeric_data[col2])
                    if abs(corr) > 0.3:  # Threshold for significance
                        graph.add_edge(col1, col2, weight=abs(corr))
        
        return graph

class DoubleMachineLearning:
    """Double Machine Learning for causal effect estimation"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
    
    def estimate_causal_effect(self, X: pd.DataFrame, treatment: pd.Series, 
                             outcome: pd.Series, confounders: List[str]) -> Dict[str, float]:
        """
        Estimate causal effect using Double Machine Learning
        
        Args:
            X: Covariate matrix including confounders
            treatment: Treatment variable
            outcome: Outcome variable
            confounders: List of confounder variable names
            
        Returns:
            Dictionary with causal effect estimates
        """
        try:
            # Prepare data
            confounders_data = X[confounders]
            
            # Stage 1: Predict treatment using confounders
            treatment_model = RandomForestRegressor(random_state=self.random_state)
            treatment_model.fit(confounders_data, treatment)
            treatment_residuals = treatment - treatment_model.predict(confounders_data)
            
            # Stage 2: Predict outcome using confounders
            outcome_model = RandomForestRegressor(random_state=self.random_state)
            outcome_model.fit(confounders_data, outcome)
            outcome_residuals = outcome - outcome_model.predict(confounders_data)
            
            # Stage 3: Estimate causal effect using residuals
            causal_model = LinearRegression()
            causal_model.fit(treatment_residuals.values.reshape(-1, 1), outcome_residuals)
            
            causal_effect = causal_model.coef_[0]
            
            # Calculate standard error using bootstrap
            bootstrap_effects = []
            n_bootstrap = 100
            n_samples = len(treatment_residuals)
            
            for _ in range(n_bootstrap):
                indices = np.random.choice(n_samples, size=n_samples, replace=True)
                boot_treatment = treatment_residuals.iloc[indices]
                boot_outcome = outcome_residuals[indices]
                
                boot_model = LinearRegression()
                boot_model.fit(boot_treatment.values.reshape(-1, 1), boot_outcome)
                bootstrap_effects.append(boot_model.coef_[0])
            
            std_error = np.std(bootstrap_effects)
            
            # Calculate confidence interval
            ci_lower = np.percentile(bootstrap_effects, 2.5)
            ci_upper = np.percentile(bootstrap_effects, 97.5)
            
            # T-test for significance
            t_stat = causal_effect / (std_error + 1e-8)
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n_samples-1))
            
            return {
                'causal_effect': causal_effect,
                'std_error': std_error,
                'p_value': p_value,
                't_statistic': t_stat,
                'ci_lower': ci_lower,
                'ci_upper': ci_upper,
                'is_significant': p_value < 0.05
            }
            
        except Exception as e:
            logger.error(f"DML estimation failed: {str(e)}")
            return {
                'causal_effect': 0.0,
                'std_error': 0.0,
                'p_value': 1.0,
                't_statistic': 0.0,
                'ci_lower': 0.0,
                'ci_upper': 0.0,
                'is_significant': False
            }

class NonLinearCausalModels:
    """Non-linear causal relationship modeling"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
    
    def polynomial_causal_model(self, X: pd.DataFrame, y: pd.Series, 
                               degree: int = 2) -> Dict[str, Any]:
        """
        Fit polynomial causal model to capture non-linear relationships
        
        Args:
            X: Feature matrix
            y: Target variable
            degree: Polynomial degree
            
        Returns:
            Dictionary with model results
        """
        try:
            # Create polynomial features
            poly = PolynomialFeatures(degree=degree, include_bias=False)
            X_poly = poly.fit_transform(X)
            
            # Fit regularized polynomial model
            model = LassoCV(cv=5, random_state=self.random_state)
            model.fit(X_poly, y)
            
            # Get feature names
            feature_names = poly.get_feature_names_out(X.columns)
            
            # Calculate feature importance
            coefficients = model.coef_
            feature_importance = dict(zip(feature_names, np.abs(coefficients)))
            
            # Calculate R-squared
            y_pred = model.predict(X_poly)
            r2 = r2_score(y, y_pred)
            
            # Identify non-linear terms
            nonlinear_terms = {}
            for i, name in enumerate(feature_names):
                if '^' in name or ' ' in name:  # Interaction or power terms
                    nonlinear_terms[name] = coefficients[i]
            
            return {
                'model': model,
                'polynomial_transformer': poly,
                'feature_importance': feature_importance,
                'nonlinear_terms': nonlinear_terms,
                'r_squared': r2,
                'linear_score': self._calculate_linear_score(X, y),
                'nonlinearity_gain': r2 - self._calculate_linear_score(X, y)
            }
            
        except Exception as e:
            logger.error(f"Polynomial causal model failed: {str(e)}")
            return {'error': str(e)}
    
    def _calculate_linear_score(self, X: pd.DataFrame, y: pd.Series) -> float:
        """Calculate R-squared for linear model"""
        try:
            linear_model = LinearRegression()
            linear_model.fit(X, y)
            y_pred = linear_model.predict(X)
            return r2_score(y, y_pred)
        except:
            return 0.0
    
    def kernel_causal_analysis(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        Use kernel methods for non-linear causal analysis
        
        Args:
            X: Feature matrix
            y: Target variable
            
        Returns:
            Dictionary with kernel-based results
        """
        try:
            from sklearn.kernel_ridge import KernelRidge
            from sklearn.model_selection import GridSearchCV
            
            # Grid search for optimal kernel parameters
            param_grid = {
                'kernel': ['rbf', 'polynomial'],
                'alpha': [0.1, 1.0, 10.0],
                'gamma': [0.1, 1.0, 10.0]
            }
            
            kernel_model = GridSearchCV(
                KernelRidge(), 
                param_grid, 
                cv=5, 
                scoring='r2'
            )
            
            kernel_model.fit(X, y)
            
            # Get best model performance
            best_score = kernel_model.best_score_
            linear_score = self._calculate_linear_score(X, y)
            
            return {
                'kernel_score': best_score,
                'linear_score': linear_score,
                'nonlinearity_benefit': best_score - linear_score,
                'best_kernel': kernel_model.best_params_.get('kernel', 'rbf'),
                'best_alpha': kernel_model.best_params_.get('alpha', 1.0)
            }
            
        except ImportError:
            logger.warning("Scikit-learn kernel methods not fully available")
            return self._simple_nonlinear_test(X, y)
        except Exception as e:
            logger.warning(f"Kernel analysis failed: {str(e)}")
            return self._simple_nonlinear_test(X, y)
    
    def _simple_nonlinear_test(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Simple non-linearity test using random forest"""
        try:
            # Compare random forest vs linear regression
            rf_model = RandomForestRegressor(n_estimators=100, random_state=self.random_state)
            rf_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')
            
            linear_model = LinearRegression()
            linear_scores = cross_val_score(linear_model, X, y, cv=5, scoring='r2')
            
            return {
                'rf_score': np.mean(rf_scores),
                'linear_score': np.mean(linear_scores),
                'nonlinearity_benefit': np.mean(rf_scores) - np.mean(linear_scores)
            }
        except:
            return {'rf_score': 0.0, 'linear_score': 0.0, 'nonlinearity_benefit': 0.0}

class AdvancedInterventionAnalysis:
    """Advanced intervention analysis with change point detection"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
    
    def detect_change_points(self, time_series: pd.Series, 
                           method: str = 'cusum') -> List[int]:
        """
        Detect change points in time series
        
        Args:
            time_series: Time series data
            method: Change point detection method
            
        Returns:
            List of change point indices
        """
        try:
            if method == 'cusum':
                return self._cusum_change_points(time_series)
            elif method == 'peaks':
                return self._peak_based_change_points(time_series)
            else:
                return self._variance_change_points(time_series)
                
        except Exception as e:
            logger.warning(f"Change point detection failed: {str(e)}")
            return []
    
    def _cusum_change_points(self, series: pd.Series, threshold: float = 2.0) -> List[int]:
        """CUSUM-based change point detection"""
        # Calculate CUSUM
        mean_val = series.mean()
        std_val = series.std()
        
        if std_val == 0:
            return []
        
        normalized = (series - mean_val) / std_val
        cusum_pos = np.zeros(len(series))
        cusum_neg = np.zeros(len(series))
        
        for i in range(1, len(series)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + normalized.iloc[i] - 0.5)
            cusum_neg[i] = min(0, cusum_neg[i-1] + normalized.iloc[i] + 0.5)
        
        # Find change points
        change_points = []
        for i in range(len(series)):
            if abs(cusum_pos[i]) > threshold or abs(cusum_neg[i]) > threshold:
                change_points.append(i)
        
        return change_points
    
    def _peak_based_change_points(self, series: pd.Series) -> List[int]:
        """Peak-based change point detection"""
        # Calculate first difference
        diff_series = series.diff().abs()
        
        # Find peaks in the difference series
        threshold = diff_series.quantile(0.9)  # Top 10% of changes
        peaks, _ = find_peaks(diff_series.values, height=threshold)
        
        return peaks.tolist()
    
    def _variance_change_points(self, series: pd.Series, window: int = 10) -> List[int]:
        """Variance-based change point detection"""
        if len(series) < 2 * window:
            return []
        
        # Calculate rolling variance
        rolling_var = series.rolling(window=window).var()
        
        # Find significant changes in variance
        var_changes = rolling_var.diff().abs()
        threshold = var_changes.quantile(0.9)
        
        change_points = []
        for i, change in enumerate(var_changes):
            if change > threshold and not np.isnan(change):
                change_points.append(i)
        
        return change_points
    
    def synthetic_control_analysis(self, treatment_unit: pd.Series, 
                                 control_units: pd.DataFrame,
                                 treatment_start: int) -> Dict[str, Any]:
        """
        Synthetic control method for causal inference
        
        Args:
            treatment_unit: Time series for treated unit
            control_units: DataFrame with control unit time series
            treatment_start: Index where treatment begins
            
        Returns:
            Dictionary with synthetic control results
        """
        try:
            # Pre-treatment period
            pre_treatment_treated = treatment_unit[:treatment_start]
            pre_treatment_controls = control_units.iloc[:treatment_start]
            
            # Post-treatment period
            post_treatment_treated = treatment_unit[treatment_start:]
            post_treatment_controls = control_units.iloc[treatment_start:]
            
            # Find optimal weights for synthetic control
            # Using ridge regression to find weights
            ridge = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0])
            ridge.fit(pre_treatment_controls.T, pre_treatment_treated)
            
            # Ensure weights are non-negative and sum to 1
            weights = np.maximum(ridge.coef_, 0)
            weights = weights / (weights.sum() + 1e-8)
            
            # Construct synthetic control
            synthetic_pre = pre_treatment_controls @ weights
            synthetic_post = post_treatment_controls @ weights
            
            # Calculate treatment effect
            pre_treatment_diff = pre_treatment_treated - synthetic_pre
            post_treatment_diff = post_treatment_treated - synthetic_post
            
            # Average treatment effect
            ate = post_treatment_diff.mean()
            
            # Calculate R-squared for pre-treatment fit
            pre_treatment_r2 = r2_score(pre_treatment_treated, synthetic_pre)
            
            # Calculate cumulative effect
            cumulative_effect = post_treatment_diff.sum()
            
            return {
                'average_treatment_effect': ate,
                'cumulative_effect': cumulative_effect,
                'pre_treatment_r2': pre_treatment_r2,
                'weights': dict(zip(control_units.columns, weights)),
                'post_treatment_effects': post_treatment_diff.tolist(),
                'synthetic_control_pre': synthetic_pre.tolist(),
                'synthetic_control_post': synthetic_post.tolist()
            }
            
        except Exception as e:
            logger.error(f"Synthetic control analysis failed: {str(e)}")
            return {
                'average_treatment_effect': 0.0,
                'cumulative_effect': 0.0,
                'pre_treatment_r2': 0.0,
                'error': str(e)
            }

class TimeVaryingCausalEffects:
    """Analysis of time-varying causal effects"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
    
    def rolling_causal_effects(self, treatment: pd.Series, outcome: pd.Series,
                             confounders: pd.DataFrame, window: int = 12) -> Dict[str, List[float]]:
        """
        Estimate time-varying causal effects using rolling windows
        
        Args:
            treatment: Treatment time series
            outcome: Outcome time series
            confounders: Time-varying confounders
            window: Rolling window size
            
        Returns:
            Dictionary with time-varying effect estimates
        """
        effects = []
        p_values = []
        times = []
        
        for i in range(window, len(treatment)):
            # Extract window data
            treat_window = treatment.iloc[i-window:i]
            outcome_window = outcome.iloc[i-window:i]
            confounders_window = confounders.iloc[i-window:i]
            
            try:
                # Simple causal effect estimation within window
                # Control for confounders using residualization
                confounder_model = LinearRegression()
                
                # Residualize treatment
                confounder_model.fit(confounders_window, treat_window)
                treat_residual = treat_window - confounder_model.predict(confounders_window)
                
                # Residualize outcome
                confounder_model.fit(confounders_window, outcome_window)
                outcome_residual = outcome_window - confounder_model.predict(confounders_window)
                
                # Estimate causal effect
                causal_model = LinearRegression()
                causal_model.fit(treat_residual.values.reshape(-1, 1), outcome_residual)
                
                effect = causal_model.coef_[0]
                
                # Calculate p-value using correlation test
                corr, p_val = stats.pearsonr(treat_residual, outcome_residual)
                
                effects.append(effect)
                p_values.append(p_val)
                times.append(i)
                
            except Exception as e:
                logger.warning(f"Rolling effect estimation failed at time {i}: {str(e)}")
                effects.append(0.0)
                p_values.append(1.0)
                times.append(i)
        
        return {
            'effects': effects,
            'p_values': p_values,
            'times': times,
            'significant_effects': [e for e, p in zip(effects, p_values) if p < 0.05]
        }
    
    def regime_switching_analysis(self, data: pd.DataFrame, 
                                target_col: str, n_regimes: int = 2) -> Dict[str, Any]:
        """
        Analyze regime-switching causal effects
        
        Args:
            data: DataFrame with all variables
            target_col: Target variable column name
            n_regimes: Number of regimes to identify
            
        Returns:
            Dictionary with regime-specific results
        """
        try:
            # Simple regime identification using k-means on target variable variance
            from sklearn.cluster import KMeans
            
            # Calculate rolling variance of target
            target_var = data[target_col].rolling(window=6).var().dropna()
            
            # Identify regimes
            kmeans = KMeans(n_clusters=n_regimes, random_state=self.random_state)
            regimes = kmeans.fit_predict(target_var.values.reshape(-1, 1))
            
            # Analyze each regime
            regime_results = {}
            
            for regime in range(n_regimes):
                regime_mask = regimes == regime
                regime_data = data.iloc[regime_mask]
                
                if len(regime_data) < 10:
                    continue
                
                # Calculate correlations within regime
                regime_corrs = {}
                for col in data.columns:
                    if col != target_col and col in regime_data.columns:
                        corr = regime_data[col].corr(regime_data[target_col])
                        regime_corrs[col] = corr if not np.isnan(corr) else 0.0
                
                regime_results[f'regime_{regime}'] = {
                    'correlations': regime_corrs,
                    'n_observations': len(regime_data),
                    'target_mean': regime_data[target_col].mean(),
                    'target_variance': regime_data[target_col].var()
                }
            
            return regime_results
            
        except Exception as e:
            logger.error(f"Regime switching analysis failed: {str(e)}")
            return {'error': str(e)}

class CausalIndicatorIntegration:
    """
    Advanced causal indicator integration combining all sophisticated methods
    """
    
    def __init__(self, config_manager, data_loader, indicator_analyzer):
        """
        Initialize the AdvancedCausalIntegration
        
        Args:
            config_manager: Configuration manager instance
            data_loader: Data loader instance
            indicator_analyzer: Enhanced indicator analyzer instance
        """
        self.config_manager = config_manager
        self.data_loader = data_loader
        self.indicator_analyzer = indicator_analyzer
        
        # Initialize analysis engines
        self.causal_discovery = CausalDiscoveryEngine()
        self.dml_engine = DoubleMachineLearning()
        self.nonlinear_models = NonLinearCausalModels()
        self.intervention_analyzer = AdvancedInterventionAnalysis()
        self.time_varying_analyzer = TimeVaryingCausalEffects()
        
        # Storage for results
        self.causal_graph = None
        self.causal_effects = {}
        self.nonlinear_effects = {}
        self.intervention_results = {}
        self.time_varying_results = {}
        
        # Advanced parameters
        self.causal_params = {
            'discovery_method': 'pc',  # 'pc', 'lingam', 'both'
            'enable_dml': True,
            'enable_nonlinear': True,
            'enable_interventions': True,
            'enable_time_varying': True,
            'polynomial_degree': 2,
            'rolling_window': 12,
            'n_regimes': 2,
            'bootstrap_samples': 100,
            'alpha': 0.05
        }
        
        # Load parameters from config
        self._load_config_params()
    
    def _load_config_params(self) -> None:
        """Load advanced causal parameters from configuration"""
        causal_config = self.config_manager.get_value('indicators.advanced_causal', {})
        
        if causal_config:
            for key, value in causal_config.items():
                if key in self.causal_params:
                    self.causal_params[key] = value
                    
            logger.info("Loaded advanced causal parameters from configuration")
    
    def comprehensive_causal_analysis(self) -> Dict[str, Dict[str, Any]]:
        """
        Perform comprehensive causal analysis using all advanced methods
        
        Returns:
            Dictionary with comprehensive causal analysis results
        """
        logger.info("ðŸ”¬ Starting comprehensive causal analysis")
        
        # Load and prepare data
        country_data = self.data_loader.load_country_historical()
        all_indicators = self.data_loader.load_all_indicators()
        
        if all_indicators.empty:
            logger.warning("No indicator data available for causal analysis")
            return {}
        
        # Get configuration
        indicator_configs = self.config_manager.get_indicators()
        column_mapping = self.config_manager.get_column_mapping('country_historical')
        id_col = column_mapping.get('id_column', 'idGeo')
        
        # Prepare panel data
        analysis_data = self._prepare_causal_panel_data(
            country_data, all_indicators, indicator_configs, id_col
        )
        
        if analysis_data.empty:
            logger.warning("No valid causal analysis data")
            return {}
        
        # Extract variables
        indicator_names = [ind['name'] for ind in indicator_configs]
        indicator_cols = [col for col in analysis_data.columns 
                         if any(ind_name in col for ind_name in indicator_names)]
        
        # 1. Causal Discovery
        logger.info("ðŸ” Performing causal discovery")
        discovery_results = self._perform_causal_discovery(analysis_data, indicator_cols)
        
        # 2. Double Machine Learning
        if self.causal_params['enable_dml']:
            logger.info("ðŸ¤– Running Double Machine Learning")
            dml_results = self._perform_dml_analysis(analysis_data, indicator_cols)
        else:
            dml_results = {}
        
        # 3. Non-linear causal models
        if self.causal_params['enable_nonlinear']:
            logger.info("ðŸ“ˆ Analyzing non-linear causal relationships")
            nonlinear_results = self._analyze_nonlinear_causality(analysis_data, indicator_cols)
        else:
            nonlinear_results = {}
        
        # 4. Advanced intervention analysis
        if self.causal_params['enable_interventions']:
            logger.info("ðŸ’‰ Performing advanced intervention analysis")
            intervention_results = self._advanced_intervention_analysis(analysis_data, indicator_cols)
        else:
            intervention_results = {}
        
        # 5. Time-varying causal effects
        if self.causal_params['enable_time_varying']:
            logger.info("â° Analyzing time-varying causal effects")
            time_varying_results = self._analyze_time_varying_effects(analysis_data, indicator_cols)
        else:
            time_varying_results = {}
        
        # Combine all results
        comprehensive_results = {}
        
        for ind_name in indicator_names:
            related_cols = [col for col in indicator_cols if ind_name in col]
            main_col = ind_name if ind_name in indicator_cols else (related_cols[0] if related_cols else None)
            
            if main_col is None:
                continue
            
            comprehensive_results[ind_name] = {
                'causal_discovery': discovery_results.get(main_col, {}),
                'dml_effects': dml_results.get(main_col, {}),
                'nonlinear_effects': nonlinear_results.get(main_col, {}),
                'intervention_effects': intervention_results.get(main_col, {}),
                'time_varying_effects': time_varying_results.get(main_col, {}),
                'composite_causal_strength': self._calculate_composite_strength(
                    discovery_results.get(main_col, {}),
                    dml_results.get(main_col, {}),
                    nonlinear_results.get(main_col, {}),
                    intervention_results.get(main_col, {}),
                    time_varying_results.get(main_col, {})
                )
            }
        
        # Store results
        self.causal_effects = {k: v['composite_causal_strength'] for k, v in comprehensive_results.items()}
        
        # Generate comprehensive visualizations
        self._generate_causal_visualizations(comprehensive_results)
        
        # Generate detailed report
        self._generate_causal_report(comprehensive_results)
        
        logger.info("âœ… Comprehensive causal analysis completed")
        return comprehensive_results
    
    def _prepare_causal_panel_data(self, country_data: pd.DataFrame, 
                                 indicators_data: pd.DataFrame, 
                                 indicator_configs: List[Dict[str, Any]], 
                                 id_col: str) -> pd.DataFrame:
        """Prepare panel data for causal analysis"""
        # Similar to enhanced analyzer but with additional preprocessing
        # Get common years
        country_years = set(country_data['Year'].unique())
        indicator_years = set(indicators_data['Year'].unique())
        common_years = sorted(country_years.intersection(indicator_years))
        
        if len(common_years) < 3:  # Need at least 3 years for causal analysis
            logger.warning("Insufficient time periods for causal analysis")
            return pd.DataFrame()
        
        # Start with country data
        panel_data = country_data[country_data['Year'].isin(common_years)].copy()
        panel_data = panel_data[[id_col, 'Year', 'Value']].copy()
        panel_data.rename(columns={'Value': 'market_value'}, inplace=True)
        
        # Add indicators
        for indicator_config in indicator_configs:
            indicator_name = indicator_config.get('name')
            indicator_df = indicators_data[indicators_data['Indicator'] == indicator_name]
            
            if indicator_df.empty:
                continue
            
            # Filter for common years
            indicator_df = indicator_df[indicator_df['Year'].isin(common_years)]
            
            if indicator_df.empty:
                continue
            
            # Merge
            panel_data = pd.merge(
                panel_data,
                indicator_df[[id_col, 'Year', 'Value']],
                on=[id_col, 'Year'],
                how='left',
                suffixes=('', f'_{indicator_name}')
            )
            
            panel_data.rename(columns={'Value': indicator_name}, inplace=True)
        
        # Handle missing values and ensure sufficient data
        panel_data = panel_data.dropna(subset=['market_value'])
        
        # Require at least 20 observations for meaningful causal analysis
        if len(panel_data) < 20:
            logger.warning("Insufficient observations for causal analysis")
            return pd.DataFrame()
        
        return panel_data
    
    def _perform_causal_discovery(self, data: pd.DataFrame, 
                                indicator_cols: List[str]) -> Dict[str, Dict[str, Any]]:
        """Perform causal discovery analysis"""
        results = {}
        
        # Prepare data for causal discovery
        discovery_data = data[indicator_cols + ['market_value']].select_dtypes(include=[np.number])
        discovery_data = discovery_data.dropna()
        
        if len(discovery_data) < 10:
            return results
        
        try:
            # PC Algorithm
            if self.causal_params['discovery_method'] in ['pc', 'both']:
                pc_graph = self.causal_discovery.pc_algorithm(discovery_data, self.causal_params['alpha'])
                
                # Analyze connections to market_value
                for col in indicator_cols:
                    if col in pc_graph.nodes and pc_graph.has_edge(col, 'market_value'):
                        edge_data = pc_graph.get_edge_data(col, 'market_value')
                        results[col] = results.get(col, {})
                        results[col]['pc_connected'] = True
                        results[col]['pc_weight'] = edge_data.get('weight', 1.0)
                    else:
                        results[col] = results.get(col, {})
                        results[col]['pc_connected'] = False
                        results[col]['pc_weight'] = 0.0
            
            # LiNGAM
            if self.causal_params['discovery_method'] in ['lingam', 'both']:
                lingam_graph = self.causal_discovery.lingam_discovery(discovery_data)
                
                # Analyze directed connections to market_value
                for col in indicator_cols:
                    if col in lingam_graph.nodes and lingam_graph.has_edge(col, 'market_value'):
                        edge_data = lingam_graph.get_edge_data(col, 'market_value')
                        results[col] = results.get(col, {})
                        results[col]['lingam_connected'] = True
                        results[col]['lingam_weight'] = edge_data.get('weight', 1.0)
                    else:
                        results[col] = results.get(col, {})
                        results[col]['lingam_connected'] = False
                        results[col]['lingam_weight'] = 0.0
                
                # Store the causal graph
                self.causal_graph = lingam_graph
        
        except Exception as e:
            logger.warning(f"Causal discovery failed: {str(e)}")
        
        return results
    
    def _perform_dml_analysis(self, data: pd.DataFrame, 
                            indicator_cols: List[str]) -> Dict[str, Dict[str, Any]]:
        """Perform Double Machine Learning analysis"""
        results = {}
        
        for target_col in indicator_cols:
            if target_col not in data.columns:
                continue
            
            # Use other indicators as confounders
            confounders = [col for col in indicator_cols if col != target_col and col in data.columns]
            
            if len(confounders) < 2:
                continue
            
            # Prepare data
            dml_data = data[[target_col] + confounders + ['market_value']].dropna()
            
            if len(dml_data) < 20:
                continue
            
            try:
                dml_result = self.dml_engine.estimate_causal_effect(
                    X=dml_data[confounders + [target_col]],
                    treatment=dml_data[target_col],
                    outcome=dml_data['market_value'],
                    confounders=confounders
                )
                
                results[target_col] = dml_result
                
            except Exception as e:
                logger.warning(f"DML analysis failed for {target_col}: {str(e)}")
                results[target_col] = {'causal_effect': 0.0, 'is_significant': False}
        
        return results
    
    def _analyze_nonlinear_causality(self, data: pd.DataFrame, 
                                   indicator_cols: List[str]) -> Dict[str, Dict[str, Any]]:
        """Analyze non-linear causal relationships"""
        results = {}
        
        # Prepare feature matrix
        X = data[indicator_cols].select_dtypes(include=[np.number]).dropna()
        y = data['market_value'].dropna()
        
        # Align X and y
        common_idx = X.index.intersection(y.index)
        X = X.loc[common_idx]
        y = y.loc[common_idx]
        
        if len(X) < 20:
            return results
        
        try:
            # Polynomial causal model
            poly_results = self.nonlinear_models.polynomial_causal_model(
                X, y, degree=self.causal_params['polynomial_degree']
            )
            
            # Kernel causal analysis
            kernel_results = self.nonlinear_models.kernel_causal_analysis(X, y)
            
            # Extract results for each indicator
            for col in indicator_cols:
                if col in X.columns:
                    results[col] = {
                        'polynomial_importance': poly_results.get('feature_importance', {}).get(col, 0.0),
                        'nonlinear_terms': {k: v for k, v in poly_results.get('nonlinear_terms', {}).items() if col in k},
                        'nonlinearity_gain': poly_results.get('nonlinearity_gain', 0.0),
                        'kernel_benefit': kernel_results.get('nonlinearity_benefit', 0.0)
                    }
        
        except Exception as e:
            logger.warning(f"Non-linear analysis failed: {str(e)}")
        
        return results
    
    def _advanced_intervention_analysis(self, data: pd.DataFrame, 
                                      indicator_cols: List[str]) -> Dict[str, Dict[str, Any]]:
        """Perform advanced intervention analysis"""
        results = {}
        id_col = self.config_manager.get_column_mapping('country_historical').get('id_column', 'idGeo')
        
        for col in indicator_cols:
            if col not in data.columns:
                continue
            
            col_results = {}
            
            # Change point detection for each country
            country_interventions = []
            
            for country_id in data[id_col].unique():
                country_data = data[data[id_col] == country_id].sort_values('Year')
                
                if len(country_data) < 10:
                    continue
                
                # Detect change points in indicator
                change_points = self.intervention_analyzer.detect_change_points(
                    country_data[col], method='cusum'
                )
                
                if not change_points:
                    continue
                
                # Analyze interventions
                for cp in change_points:
                    if cp < 3 or cp >= len(country_data) - 3:
                        continue  # Need sufficient pre/post data
                    
                    # Synthetic control analysis
                    treatment_series = country_data['market_value'].reset_index(drop=True)
                    
                    # Use other countries as controls
                    other_countries = data[data[id_col] != country_id]
                    if len(other_countries) == 0:
                        continue
                    
                    # Create control matrix (simplified)
                    control_data = other_countries.groupby('Year')['market_value'].mean()
                    
                    # Align treatment and control series
                    if len(control_data) >= len(treatment_series):
                        control_matrix = pd.DataFrame({
                            'control': control_data.iloc[:len(treatment_series)]
                        })
                        
                        try:
                            sc_result = self.intervention_analyzer.synthetic_control_analysis(
                                treatment_series, control_matrix, cp
                            )
                            
                            country_interventions.append({
                                'country': country_id,
                                'change_point': cp,
                                'treatment_effect': sc_result.get('average_treatment_effect', 0),
                                'pre_treatment_r2': sc_result.get('pre_treatment_r2', 0)
                            })
                            
                        except Exception as e:
                            logger.warning(f"Synthetic control failed for {country_id}: {str(e)}")
            
            # Aggregate intervention results
            if country_interventions:
                effects = [ci['treatment_effect'] for ci in country_interventions]
                col_results['mean_intervention_effect'] = np.mean(effects)
                col_results['intervention_effects_std'] = np.std(effects)
                col_results['n_interventions'] = len(country_interventions)
                col_results['significant_interventions'] = sum(1 for e in effects if abs(e) > np.std(effects))
            else:
                col_results['mean_intervention_effect'] = 0.0
                col_results['n_interventions'] = 0
            
            results[col] = col_results
        
        return results
    
    def _analyze_time_varying_effects(self, data: pd.DataFrame, 
                                    indicator_cols: List[str]) -> Dict[str, Dict[str, Any]]:
        """Analyze time-varying causal effects"""
        results = {}
        
        for col in indicator_cols:
            if col not in data.columns:
                continue
            
            col_results = {}
            
            # Sort data by time
            time_sorted = data.sort_values('Year')
            
            if len(time_sorted) < 24:  # Need sufficient data for rolling analysis
                continue
            
            try:
                # Prepare other indicators as confounders
                confounders = [c for c in indicator_cols if c != col and c in data.columns]
                
                if len(confounders) >= 1:
                    rolling_results = self.time_varying_analyzer.rolling_causal_effects(
                        treatment=time_sorted[col],
                        outcome=time_sorted['market_value'],
                        confounders=time_sorted[confounders],
                        window=self.causal_params['rolling_window']
                    )
                    
                    col_results['rolling_effects'] = rolling_results['effects']
                    col_results['effect_stability'] = 1 - (np.std(rolling_results['effects']) / 
                                                          (abs(np.mean(rolling_results['effects'])) + 1e-8))
                    col_results['n_significant_periods'] = len(rolling_results['significant_effects'])
                
                # Regime switching analysis
                regime_results = self.time_varying_analyzer.regime_switching_analysis(
                    time_sorted, 'market_value', self.causal_params['n_regimes']
                )
                
                col_results['regime_effects'] = regime_results
                
            except Exception as e:
                logger.warning(f"Time-varying analysis failed for {col}: {str(e)}")
                col_results['effect_stability'] = 0.0
                col_results['n_significant_periods'] = 0
            
            results[col] = col_results
        
        return results
    
    def _calculate_composite_strength(self, discovery: Dict, dml: Dict, 
                                    nonlinear: Dict, intervention: Dict, 
                                    time_varying: Dict) -> float:
        """Calculate composite causal strength from all methods"""
        strength = 0.0
        
        # Discovery component (20%)
        pc_weight = discovery.get('pc_weight', 0.0)
        lingam_weight = discovery.get('lingam_weight', 0.0)
        discovery_component = (pc_weight + lingam_weight) / 2 * 0.2
        
        # DML component (30%)
        dml_effect = abs(dml.get('causal_effect', 0.0))
        is_significant = dml.get('is_significant', False)
        dml_component = dml_effect * 0.3 if is_significant else 0.0
        
        # Non-linear component (20%)
        nonlinear_gain = nonlinear.get('nonlinearity_gain', 0.0)
        kernel_benefit = nonlinear.get('kernel_benefit', 0.0)
        nonlinear_component = max(nonlinear_gain, kernel_benefit) * 0.2
        
        # Intervention component (15%)
        intervention_effect = abs(intervention.get('mean_intervention_effect', 0.0))
        n_interventions = intervention.get('n_interventions', 0)
        intervention_component = intervention_effect * 0.15 if n_interventions > 0 else 0.0
        
        # Time-varying component (15%)
        effect_stability = time_varying.get('effect_stability', 0.0)
        n_significant = time_varying.get('n_significant_periods', 0)
        time_component = effect_stability * 0.15 if n_significant > 0 else 0.0
        
        # Combine components
        strength = discovery_component + dml_component + nonlinear_component + intervention_component + time_component
        
        # Normalize to [0, 1]
        return min(1.0, max(0.0, strength))
    
    def _generate_causal_visualizations(self, results: Dict[str, Dict[str, Any]]) -> str:
        """Generate comprehensive causal visualizations"""
        output_dir = self.config_manager.config.get('output', {}).get('save_path', 'output')
        os.makedirs(output_dir, exist_ok=True)
        
        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        fig.suptitle('Advanced Causal Analysis Results', fontsize=16, fontweight='bold')
        
        # Plot 1: Composite causal strengths
        indicators = list(results.keys())
        strengths = [results[ind]['composite_causal_strength'] for ind in indicators]
        
        axes[0, 0].bar(range(len(indicators)), strengths)
        axes[0, 0].set_xticks(range(len(indicators)))
        axes[0, 0].set_xticklabels(indicators, rotation=45)
        axes[0, 0].set_title('Composite Causal Strengths')
        axes[0, 0].set_ylabel('Causal Strength')
        
        # Plot 2: DML effects
        dml_effects = []
        dml_significance = []
        for ind in indicators:
            dml_data = results[ind].get('dml_effects', {})
            dml_effects.append(dml_data.get('causal_effect', 0))
            dml_significance.append(dml_data.get('is_significant', False))
        
        colors = ['green' if sig else 'red' for sig in dml_significance]
        axes[0, 1].bar(range(len(indicators)), dml_effects, color=colors, alpha=0.7)
        axes[0, 1].set_xticks(range(len(indicators)))
        axes[0, 1].set_xticklabels(indicators, rotation=45)
        axes[0, 1].set_title('DML Causal Effects (Green=Significant)')
        axes[0, 1].set_ylabel('Effect Size')
        
        # Plot 3: Non-linearity benefits
        nonlinear_gains = []
        for ind in indicators:
            nl_data = results[ind].get('nonlinear_effects', {})
            nonlinear_gains.append(nl_data.get('nonlinearity_gain', 0))
        
        axes[0, 2].bar(range(len(indicators)), nonlinear_gains)
        axes[0, 2].set_xticks(range(len(indicators)))
        axes[0, 2].set_xticklabels(indicators, rotation=45)
        axes[0, 2].set_title('Non-linearity Benefits')
        axes[0, 2].set_ylabel('RÂ² Gain from Non-linearity')
        
        # Plot 4: Intervention effects
        intervention_effects = []
        n_interventions = []
        for ind in indicators:
            int_data = results[ind].get('intervention_effects', {})
            intervention_effects.append(abs(int_data.get('mean_intervention_effect', 0)))
            n_interventions.append(int_data.get('n_interventions', 0))
        
        # Create bubble plot
        axes[1, 0].scatter(range(len(indicators)), intervention_effects, 
                          s=[n*20 for n in n_interventions], alpha=0.6)
        axes[1, 0].set_xticks(range(len(indicators)))
        axes[1, 0].set_xticklabels(indicators, rotation=45)
        axes[1, 0].set_title('Intervention Effects (Size=N Interventions)')
        axes[1, 0].set_ylabel('Mean Effect Size')
        
        # Plot 5: Time-varying stability
        stabilities = []
        for ind in indicators:
            tv_data = results[ind].get('time_varying_effects', {})
            stabilities.append(tv_data.get('effect_stability', 0))
        
        axes[1, 1].bar(range(len(indicators)), stabilities)
        axes[1, 1].set_xticks(range(len(indicators)))
        axes[1, 1].set_xticklabels(indicators, rotation=45)
        axes[1, 1].set_title('Time-Varying Effect Stability')
        axes[1, 1].set_ylabel('Stability Score')
        
        # Plot 6: Causal network
        if self.causal_graph:
            pos = nx.spring_layout(self.causal_graph, seed=42)
            nx.draw_networkx(self.causal_graph, pos=pos, ax=axes[1, 2],
                           with_labels=True, node_color='lightblue',
                           node_size=1000, font_size=8, arrows=True)
            axes[1, 2].set_title('Discovered Causal Network')
        else:
            axes[1, 2].text(0.5, 0.5, 'No Causal Network Available', 
                           ha='center', va='center', transform=axes[1, 2].transAxes)
            axes[1, 2].set_title('Causal Network')
        
        plt.tight_layout()
        
        # Save visualization
        output_file = os.path.join(output_dir, 'advanced_causal_analysis.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Saved advanced causal visualizations to: {output_file}")
        return output_file
    
    def _generate_causal_report(self, results: Dict[str, Dict[str, Any]]) -> str:
        """Generate comprehensive causal analysis report"""
        output_dir = self.config_manager.config.get('output', {}).get('save_path', 'output')
        os.makedirs(output_dir, exist_ok=True)
        
        excel_file = os.path.join(output_dir, 'advanced_causal_analysis_report.xlsx')
        
        try:
            with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:
                # Summary sheet
                summary_data = []
                for ind_name, res in results.items():
                    summary_data.append({
                        'Indicator': ind_name,
                        'Composite Causal Strength': res['composite_causal_strength'],
                        'DML Effect': res['dml_effects'].get('causal_effect', 0),
                        'DML Significant': res['dml_effects'].get('is_significant', False),
                        'Nonlinearity Gain': res['nonlinear_effects'].get('nonlinearity_gain', 0),
                        'Intervention Effect': res['intervention_effects'].get('mean_intervention_effect', 0),
                        'Effect Stability': res['time_varying_effects'].get('effect_stability', 0)
                    })
                
                summary_df = pd.DataFrame(summary_data)
                summary_df = summary_df.sort_values('Composite Causal Strength', ascending=False)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                # Detailed DML results
                dml_data = []
                for ind_name, res in results.items():
                    dml_res = res['dml_effects']
                    dml_data.append({
                        'Indicator': ind_name,
                        'Causal Effect': dml_res.get('causal_effect', 0),
                        'Standard Error': dml_res.get('std_error', 0),
                        'P-value': dml_res.get('p_value', 1),
                        'T-statistic': dml_res.get('t_statistic', 0),
                        'CI Lower': dml_res.get('ci_lower', 0),
                        'CI Upper': dml_res.get('ci_upper', 0),
                        'Is Significant': dml_res.get('is_significant', False)
                    })
                
                dml_df = pd.DataFrame(dml_data)
                dml_df.to_excel(writer, sheet_name='DML Results', index=False)
                
                # Non-linear analysis results
                nonlinear_data = []
                for ind_name, res in results.items():
                    nl_res = res['nonlinear_effects']
                    nonlinear_data.append({
                        'Indicator': ind_name,
                        'Polynomial Importance': nl_res.get('polynomial_importance', 0),
                        'Nonlinearity Gain': nl_res.get('nonlinearity_gain', 0),
                        'Kernel Benefit': nl_res.get('kernel_benefit', 0),
                        'N Nonlinear Terms': len(nl_res.get('nonlinear_terms', {}))
                    })
                
                nonlinear_df = pd.DataFrame(nonlinear_data)
                nonlinear_df.to_excel(writer, sheet_name='Nonlinear Analysis', index=False)
                
                # Intervention analysis results
                intervention_data = []
                for ind_name, res in results.items():
                    int_res = res['intervention_effects']
                    intervention_data.append({
                        'Indicator': ind_name,
                        'Mean Intervention Effect': int_res.get('mean_intervention_effect', 0),
                        'Effect Std Dev': int_res.get('intervention_effects_std', 0),
                        'N Interventions': int_res.get('n_interventions', 0),
                        'Significant Interventions': int_res.get('significant_interventions', 0)
                    })
                
                intervention_df = pd.DataFrame(intervention_data)
                intervention_df.to_excel(writer, sheet_name='Intervention Analysis', index=False)
                
                # Time-varying effects
                time_varying_data = []
                for ind_name, res in results.items():
                    tv_res = res['time_varying_effects']
                    time_varying_data.append({
                        'Indicator': ind_name,
                        'Effect Stability': tv_res.get('effect_stability', 0),
                        'N Significant Periods': tv_res.get('n_significant_periods', 0),
                        'N Rolling Effects': len(tv_res.get('rolling_effects', []))
                    })
                
                time_varying_df = pd.DataFrame(time_varying_data)
                time_varying_df.to_excel(writer, sheet_name='Time-Varying Effects', index=False)
            
            logger.info(f"Generated advanced causal report: {excel_file}")
            return excel_file
            
        except Exception as e:
            logger.error(f"Error generating causal report: {str(e)}")
            return ""
    
    def get_causal_strengths(self) -> Dict[str, float]:
        """Get composite causal strengths"""
        return self.causal_effects
    
    def apply_causal_adjustments(self, country_df: pd.DataFrame) -> pd.DataFrame:
        """Apply advanced causal-based adjustments"""
        if not self.causal_effects:
            logger.warning("No causal effects available, running analysis first")
            self.comprehensive_causal_analysis()
        
        if not self.causal_effects:
            logger.warning("No causal effects available after analysis")
            return country_df
        
        # Get baseline indicator weights
        baseline_weights = self.indicator_analyzer.get_indicator_weights()
        
        # Combine with causal strengths
        enhanced_weights = {}
        for ind_name in baseline_weights.keys():
            baseline_weight = baseline_weights.get(ind_name, 0.0)
            causal_strength = self.causal_effects.get(ind_name, 0.0)
            
            # Weighted combination favoring causal evidence
            enhanced_weights[ind_name] = 0.3 * baseline_weight + 0.7 * causal_strength
        
        # Normalize weights
        total_weight = sum(enhanced_weights.values())
        if total_weight > 0:
            enhanced_weights = {k: v/total_weight for k, v in enhanced_weights.items()}
        
        # Apply adjustments using enhanced weights
        temp_weights = self.indicator_analyzer.indicator_weights.copy()
        self.indicator_analyzer.indicator_weights = enhanced_weights
        
        adjusted_df = self.indicator_analyzer.apply_indicator_adjustments(country_df)
        
        # Restore original weights
        self.indicator_analyzer.indicator_weights = temp_weights
        
        return adjusted_df
    
    def analyze_causal_relationships(self) -> Dict[str, Dict[str, Any]]:
        """
        Legacy method name - calls the comprehensive analysis
        """
        return self.comprehensive_causal_analysis()